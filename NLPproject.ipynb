{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLrcvAggohRp",
        "outputId": "acb1fd45-46af-44cc-c520-788d4ffb8c6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import reuters\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('reuters')\n",
        "\n",
        "corpus = reuters.sents(categories='acq')\n",
        "\n",
        "def build_bigram_model(corpus):\n",
        "    bigram_model = defaultdict(Counter)\n",
        "    for sentence in corpus:\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        for w1, w2 in ngrams(sentence, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
        "            bigram_model[w1][w2] += 1\n",
        "    return bigram_model\n",
        "\n",
        "bigram_model = build_bigram_model(corpus)\n",
        "\n",
        "def autocomplete(text, bigram_model, num_words=5):\n",
        "    words = text.split()\n",
        "    last_word = words[-1].lower()\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        if last_word in bigram_model:\n",
        "            next_word = bigram_model[last_word].most_common(1)[0][0]\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "            predictions.append(next_word)\n",
        "            last_word = next_word\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return text + \" \" + \" \".join(predictions)\n",
        "\n",
        "input_text = \"Machine learning is\"\n",
        "completed_text = autocomplete(input_text, bigram_model, num_words=3)\n",
        "print(\"Autocomplete:\", completed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i20nOSbc0DEc",
        "outputId": "b5fc4405-f816-4ba7-efc7-1f26b32c5e08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete: Machine learning is expected to buy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "custom_nltk_path = os.path.expanduser('~/custom_nltk_data')\n",
        "if not os.path.exists(custom_nltk_path):\n",
        "    os.makedirs(custom_nltk_path)\n",
        "\n",
        "\n",
        "nltk.download('punkt', download_dir=custom_nltk_path)\n",
        "nltk.download('gutenberg', download_dir=custom_nltk_path)\n",
        "\n",
        "\n",
        "nltk.data.path.append(custom_nltk_path)\n",
        "\n",
        "corpus = nltk.corpus.gutenberg.sents()\n",
        "\n",
        "def build_bigram_model(corpus):\n",
        "    bigram_model = defaultdict(Counter)\n",
        "    for sentence in corpus:\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        for w1, w2 in ngrams(sentence, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
        "            bigram_model[w1][w2] += 1\n",
        "    return bigram_model\n",
        "\n",
        "bigram_model = build_bigram_model(corpus)\n",
        "\n",
        "\n",
        "def autocomplete(text, bigram_model, num_words=5):\n",
        "    words = text.split()\n",
        "    last_word = words[-1].lower()\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        if last_word in bigram_model:\n",
        "            next_word = bigram_model[last_word].most_common(1)[0][0]\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "            predictions.append(next_word)\n",
        "            last_word = next_word\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return text + \" \" + \" \".join(predictions)\n",
        "\n",
        "\n",
        "input_text = \"machine learning is\"\n",
        "completed_text = autocomplete(input_text, bigram_model, num_words=3)\n",
        "print(\"Autocomplete:\", completed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p_T0n55n_ft",
        "outputId": "40e8b07d-ba19-4909-e318-566be5898899"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/custom_nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/custom_nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete: machine learning is the lord ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank, words\n",
        "from nltk.tag import hmm\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('words')\n",
        "\n",
        "train_sents = treebank.tagged_sents(tagset='universal')\n",
        "english_words = set(words.words())\n",
        "train_vocab = set(word.lower() for sent in train_sents for word, _ in sent)\n",
        "full_vocabulary = english_words.union(train_vocab)\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_sents)\n",
        "\n",
        "common_misspellings = {\n",
        "    'teh': 'the',\n",
        "    'quikc': 'quick',\n",
        "    'brownn': 'brown',\n",
        "    'fxo': 'fox',\n",
        "    'jupms': 'jumps',\n",
        "    'lazzy': 'lazy',\n",
        "}\n",
        "\n",
        "def is_correct(word):\n",
        "    return word.lower() in full_vocabulary\n",
        "\n",
        "def suggest_corrections(word, candidates, max_distance=2):\n",
        "    suggestions = [candidate for candidate in candidates if edit_distance(word, candidate) <= max_distance]\n",
        "    if not suggestions:\n",
        "        suggestions = [candidate for candidate in candidates if edit_distance(word, candidate) <= max_distance + 1]\n",
        "    return min(suggestions, key=lambda candidate: edit_distance(word, candidate), default=word)\n",
        "\n",
        "def hmm_spell_checker(sentence, tagger, common_misspellings):\n",
        "    corrected_sentence = []\n",
        "\n",
        "    for i, word in enumerate(sentence):\n",
        "        if not is_correct(word):\n",
        "            correction = common_misspellings.get(word.lower(), None)\n",
        "            if correction:\n",
        "                corrected_sentence.append(correction)\n",
        "                print(f\"Correcting '{word}' to '{correction}'\")\n",
        "            else:\n",
        "                suggested_word = suggest_corrections(word, full_vocabulary, max_distance=2)\n",
        "                corrected_sentence.append(suggested_word)\n",
        "                print(f\"Suggesting '{suggested_word}' for '{word}'\")\n",
        "        else:\n",
        "            corrected_sentence.append(word)\n",
        "\n",
        "    return \" \".join(corrected_sentence)\n",
        "\n",
        "input_sentence = ['The', 'quikc', 'brownn', 'fxo', 'jupms', 'over', 'the', 'lazzy', 'dog', 'cta', 'cet','catt']\n",
        "corrected_text = hmm_spell_checker(input_sentence, tagger, common_misspellings)\n",
        "print(\"\\nCorrected Sentence:\")\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uCsjDeNYXy",
        "outputId": "0d75f708-f49a-42c6-d5ef-be7e95337656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcting 'quikc' to 'quick'\n",
            "Correcting 'brownn' to 'brown'\n",
            "Correcting 'fxo' to 'fox'\n",
            "Correcting 'jupms' to 'jumps'\n",
            "Correcting 'lazzy' to 'lazy'\n",
            "Suggesting 'eta' for 'cta'\n",
            "Suggesting 'cest' for 'cet'\n",
            "Suggesting 'batt' for 'catt'\n",
            "\n",
            "Corrected Sentence:\n",
            "The quick brown fox jumps over the lazy dog eta cest batt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "psARxXKSsoGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}